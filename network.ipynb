{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes de Camada Única\n",
    "\n",
    "$$f: \\mathbb{R}^{m \\times n} \\rightarrow \\mathbb{R}^{m \\times o}$$\n",
    "\n",
    "Em uma rede de camada única, temos duas matrizes $x$ e $y$ de valores. A matriz $x$ possui as dimensões de $m$ por $n$, onde $m$ diz sobre a quantidade de instâncias (linhas) e $n$ a quantidade de entradas. A matriz $y$, por sua vez, possui a mesma quantidade de instâncias de $x$, porém a quantidade de saídas pode ser diferente, sendo definida como $o$.\n",
    "\n",
    "Dessa forma, para cada $x^{m \\times n}$, existe um $y^{m \\times o}$:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "x_{11} & x_{12} & x_{13} & x_{14}\\\\\n",
    "x_{21} & x_{22} & x_{23} & x_{24}\\\\\n",
    "x_{31} & x_{32} & x_{33} & x_{44}\n",
    "\\end{bmatrix}\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "y_{11} & y_{12}\\\\\n",
    "y_{21} & y_{22}\\\\\n",
    "y_{31} & y_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Neste caso\n",
    "\n",
    "$$m = 3 ; n = 4; o = 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema a ser resolvido\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png\" height=\"200\" width=\"450\" />\n",
    "\n",
    "<img src=\"https://ars.els-cdn.com/content/image/3-s2.0-B9780128147610000034-f03-01-9780128147610.jpg?_\" height=\"250\" />\n",
    "</div>\n",
    "\n",
    "A flor Íris possui três tipos:\n",
    "- Íris setosa\n",
    "- Íris versicolor\n",
    "- Íris virgínica\n",
    "\n",
    "Para classificar a flor, são utilizadas variáveis como a largura e tamanho da pétala e sépala. Dessa forma, o conjunto de valores $x$ se dá por:\n",
    "\n",
    "$$x = [sl, sw, pl, pw]$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $sl \\in \\mathbb{R^{*}_{+}}$: Tamanho da sépala\n",
    "- $sw \\in \\mathbb{R^{*}_{+}}$: Largura da sépala\n",
    "- $pl \\in \\mathbb{R^{*}_{+}}$: Largura da pétala\n",
    "- $pw \\in \\mathbb{R^{*}_{+}}$: Tamanho da pétala\n",
    "\n",
    "A partir desse valor de $x$, é gerado três tipos de valores $y$ para identificar o tipo da íris:\n",
    "\n",
    "$$y = [ise, ive, ivi]$$\n",
    "\n",
    "Onde:\n",
    "\n",
    "- $ise \\in \\{0, 1\\}$: Íris setosa\n",
    "- $ive \\in \\{0, 1\\}$: Íris versicolor\n",
    "- $ivi \\in \\{0, 1\\}$: Íris virgínica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "                      names=[\"sepal_length\",\"sepal_width\", \"petal_length\", \"petal_width\", \"class\"])\n",
    "\n",
    "dataset['Iris_Setosa'] = dataset['class'] == 'Iris-setosa'\n",
    "dataset['Iris_Versicolor'] = dataset['class'] == 'Iris-versicolor'\n",
    "dataset['Iris_Virginica'] = dataset['class'] == 'Iris-virginica'\n",
    "\n",
    "X = dataset[['sepal_length','sepal_width','petal_length','petal_width']].values\n",
    "Y = dataset[['Iris_Setosa','Iris_Versicolor','Iris_Virginica']].values\n",
    "Y = np.where(Y == True, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostra de dados\n",
    "\n",
    "Uma boa prática para treinar redes neurais é separar uma amostra da população total de dados apenas para o treinamento e uma outra amostra apenas para os testes da rede.\n",
    "\n",
    "Dessa forma, separamos 30% dos dados carregados acima para teste, enquanto os outros 70% do dados serão utilizados para o treinamento da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "train = { \"x\": X_train, \"y\": Y_train, \"n\": len(X_train) }\n",
    "test  = { \"x\": X_test, \"y\": Y_test, \"n\": len(X_test) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros\n",
    "\n",
    "### Hiper-parâmetros\n",
    "\n",
    "O conjunto de hiper-parâmetros é dado por valores definidos manualmente por nós humanos.\n",
    "\n",
    "#### Taxa de aprendizado\n",
    "\n",
    "A taxa de aprendizado é uma constante real que define o tamanho dos ajustes feitos pela função de aprendizado. Valores maiores representam mudanças mais bruscas na atualização dos pesos .\n",
    "\n",
    "$$\\alpha = x \\in \\mathbb{R}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantidade de entradas, saídas e instâncias\n",
    "\n",
    "**Instâncias ($m$)**: A quantidade de instâncias diz respeito a quantidade de linhas totais que a matriz $x$ (ou $y$) tem.\n",
    "\n",
    "**Entradas ($n$)**: A quantidade de entradas é a quantidade de valores em cada linha de $x$, nesse caso são 4.\n",
    "\n",
    "**Saídas ($o$)**: A quantidade de saídas é a quantidade de valores em cada linha de $y$, nesse caso são 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "_, o = Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros\n",
    "\n",
    "Os parâmetros consistem nos valores que a rede neural vai calcular durante o seu treinamento, no caso os pesos e os termos de viés.\n",
    "\n",
    "#### Pesos ($w$)\n",
    "\n",
    "$$w \\in \\mathbb{R}^{o \\times n}$$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "w_{11} & w_{12} & ... & w_{1n}\\\\\n",
    "w_{21} & w_{22} & ... & x_{2n}\\\\\n",
    "w_{o1} & w_{o2} & ... & w_{on}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Os pesos são valores inicializados de forma aleatória e que serão atualizados por meio da função de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = [np.random.normal(0, 0.5, n) for _ in range(o)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viés ($b$)\n",
    "\n",
    "$$\n",
    "b \\in \\mathbb{R}^o\n",
    "$$\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_o\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Assim como nos pesos, os termos de bias serão atualizados conforme erros acontecidos durante a função de inferência na função de aprendizado. Porém, como o $b$ não está diretamente relacionado ao $x$, logo sua matriz tem apenas uma coluna e $o$ linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [np.random.normal(0, 0.5, 1) for _ in range(o)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de ativação $(\\sigma)$\n",
    "\n",
    "$$\\sigma(y, \\hat{y}) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "A função de ativação tem como objetivo classificar valores que foram resultados da função de inferência. Neste caso, utilizamos a função Sigmoide $\\sigma$ para realizar essa classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(x) -> float:\n",
    "  return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de custo ($L$)\n",
    "\n",
    "A função de custo visa calcular o erro $e$ entre $y$ e $\\hat{y}$.\n",
    "\n",
    "### Mean Square Error\n",
    "\n",
    "$$L(y, \\hat{y}) = \\frac{(y - \\hat{y})^2}{2}$$\n",
    "\n",
    "Neste caso, estamos utilizando a função MSE que calcula o erro médio quadrático entre os valores de $y$ e $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y: float, y_hat: float) -> float:\n",
    "  return ((y - y_hat)**2)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de inferência ($f(x)$)\n",
    "\n",
    "$$\\hat{y_{j}} = \\sum^{n}_{i=1} w_{ij} \\cdot x_{i} + b_{j}$$\n",
    "\n",
    "A função de inferência tem como objetivo replicar o comportamento da função geradora $G(x)$. Porém, como essa rede neuronal possui uma camada, o somatório se dá seguinte forma:\n",
    "\n",
    "- $j$: Índice do neurônio de saída (exemplo: $(y_1, y_2, \\ldots, y_o )$).\n",
    "- $i$: Índice do neurônio de entrada (exemplo: $( x_1, x_2, \\ldots, x_n )$).\n",
    "- $w_{ji}$: Peso que conecta o $i$-ésimo neurônio de entrada ao $j$-ésimo neurônio de saída.\n",
    "- $b_j$: Viés específico do $j$-ésimo neurônio de saída.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x: list, w: list, b: float) -> float:\n",
    "  return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de aprendizado ($F(x)$)\n",
    "\n",
    "$$F(x) =\n",
    "      \\begin{cases}\n",
    "      w_i^{t+1} = w_i^t - \\alpha \\cdot \\frac{\\partial \\varepsilon}{\\partial w_i} \\\\\n",
    "      b^{t+1} = b^t - \\alpha \\cdot \\frac{\\partial \\varepsilon}{\\partial b}\n",
    "      \\end{cases}$$\n",
    "\n",
    "A função de aprendizagem em redes neurais é baseada no ajuste dos parâmetros do modelo (como pesos e viés) para reduzir o erro entre a previsão do modelo e o valor real. O objetivo é minimizar a função de custo, que, neste caso, é a *Mean Squared Error* (MSE), através do uso do gradiente descendente.\n",
    "\n",
    "### Gradiente descendente $\\nabla$\n",
    "\n",
    "A técnica do gradiente descendente visa descobrir o impacto do erro $\\varepsilon$ em relação ao $\\hat{y}$. Porém, como essas variáveis não estão relacionas diretamente, temos que utilizar a derivada parcial.\n",
    "\n",
    "$$L(x) \\text{ ou } \\varepsilon \\leftarrow \\hat{y} \\text{ ou } f(x)  \\leftarrow w,b $$\n",
    "\n",
    "Dessa forma, para calcular o erro ($L(x) \\text{ ou } \\varepsilon$) dependemos da estimativa ($\\hat{y}$) e para calcular a estimativa dependemos dos pesos e termo de viés ($w$ e $b$). \n",
    "\n",
    "Ou seja, a derivada parcial se dá por:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\varepsilon}{\\partial\\theta} = \\frac{\\partial{\\varepsilon}}{\\partial\\sigma} \\cdot \\frac{\\partial\\sigma}{\\partial\\hat{y}} \\cdot \\frac{\\partial\\hat{y}}{\\partial\\theta}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial\\varepsilon}{\\partial\\sigma} = \\left(\\frac{(y - \\hat{y})^2}{2}\\right)' = \\left(0.5 \\cdot (y - \\hat{y})^2\\right)' = y - \\hat{y}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial\\varepsilon}{\\partial\\sigma} = \\left(\\frac{1}{1 + e^{-x}}\\right)' = \\frac{1}{1 + e^{-x}} \\cdot (1 - y)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial\\hat{y}}{\\partial\\theta} = \\left(w \\cdot x + b\\right)' = x \\cdot 1 = x\n",
    "$$\n",
    "\n",
    "Porém, a derivada dessas funções aponta para onde a função está crescendo, mas como queremos otimizar a atualização dos pesos, devemos multiplicar pelo inverso da derivada parcial:\n",
    "\n",
    "$$-\\left(\\frac{\\partial\\varepsilon}{\\partial\\theta}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide_derivate(x) -> float:\n",
    "  y = sigmoide(x)\n",
    "  return y * (1 - y)\n",
    "\n",
    "def mse_derivate(y: float, y_hat: float) -> float:\n",
    "  return y - y_hat\n",
    "\n",
    "def learn(x: list, y: float, y_hat: float, w: float, b: float, a: float) -> float:\n",
    "  error = mse(y, y_hat)\n",
    "  y_hat_activation = sigmoide(y_hat)\n",
    "  delta = mse_derivate(y, y_hat_activation) * sigmoide_derivate(y_hat)\n",
    "\n",
    "  w += a * -delta * -x\n",
    "  b += a * -delta\n",
    "\n",
    "  return w, b, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, n = train['x'], train['y'], train['n']\n",
    "\n",
    "epochs = 100\n",
    "training_errors = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  for i in range(n):\n",
    "    random = np.random.randint(n)\n",
    "    xi, yi = x[random], y[random]\n",
    "    instance_errors = []\n",
    "\n",
    "    for output in range(o):\n",
    "      y_hat = inference(xi, w[output], b[output])\n",
    "      w[output], b[output], error = learn(xi, yi[output], y_hat, w[output], b[output], a)\n",
    "      instance_errors.append(error)\n",
    "\n",
    "    training_errors.append(np.mean(instance_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, n = test['x'], test['y'], test['n']\n",
    "\n",
    "y_hat = []\n",
    "\n",
    "for instance in range(n):\n",
    "  tmp = [sigmoide(inference(x[instance], w[output], b[output])) for output in range(o)]\n",
    "  tmp = np.where(tmp == max(tmp), 1, 0)\n",
    "  y_hat.append(tmp)\n",
    "\n",
    "y_hat = np.array(y_hat)\n",
    "\n",
    "accuracy = accuracy_score(y.argmax(axis=1), y_hat.argmax(axis=1))\n",
    "\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
